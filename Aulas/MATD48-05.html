<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Fundamentos de Análise para Planejamento Experimental</title>
    <meta charset="utf-8" />
    <meta name="author" content="Raydonal Ospina" />
    <script src="libs/header-attrs-2.30/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
    <script src="libs/viz-1.8.2/viz.js"></script>
    <link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
    <script src="libs/grViz-binding-1.0.11/grViz.js"></script>
    <link rel="stylesheet" href="custom-styles.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Fundamentos de Análise para Planejamento Experimental
]
.subtitle[
## Populações Conceituais de Respostas e modelos lineares
]
.author[
### Raydonal Ospina
]

---





class: center, middle, inverse

# Populações Conceituais de Respostas
---

## Populações Conceituais de Respostas


Em planejamento de experimentos, a concepção de uma **população conceitual de respostas** é bastante útil e natural.

Considere `\(Y_{ij}\)` (`\(i=1,...,t\)`, `\(j=1,...,N\)`) como o valor **potencial** observado da variável resposta Y referente ao `\(j\)`-ésimo elemento da população, se ele fosse submetido ao tratamento `\(i\)`.

**Exemplo:** Como cada elemento da população é submetido ao efeito de apenas um dos tratamentos, tem-se, por exemplo, que o elemento `j` submetido ao tratamento `1` fornecerá a resposta `\(Y_{1j}\)`. Sendo este o caso, o valor `\(Y_{2j}\)`, por exemplo, não será observado, embora ele exista conceitualmente (se o tratamento escolhido para ser aplicado ao elemento `j` tivesse sido o de número 2, `\(Y_{2j}\)` seria o valor observado).

---
### Definição e Estrutura da População Conceitual

A **população conceitual de respostas** desse experimento é definida como o conjunto `\(\{Y_{ij}\}\)` de todas as possíveis respostas dos `\(N\)` elementos que compõem a população sob o efeito de cada um dos `\(t\)` tratamentos sob estudo.

O **Quadro ** a seguir ilustra a população conceitual de respostas em um experimento para comparar os efeitos de `\(t=2\)` tratamentos (`\(i\)` o tratamento) em uma população composta por `\(N=4\)` elementos (unidades experimentais  (UE) como `\(j\)`).

| Tratamento | UE 1 | UE 2 | UE 3 | UE 4 | Total | Média |
|:----------:|:----:|:----:|:----:|:----:|:----:|:----:|
| **i=1**    | `\(Y_{11}\)` | `\(Y_{12}\)` | `\(Y_{13}\)` | `\(Y_{14}\)` | `\(Y_{1.}\)` | `\(\bar{Y}_{1.}\)` |
| **i=2**    | `\(Y_{21}\)` | `\(Y_{22}\)` | `\(Y_{23}\)` | `\(Y_{24}\)` | `\(Y_{2.}\)` | `\(\bar{Y}_{2.}\)` |
| Total      | `\({Y}_{.1}\)` | `\({Y}_{.2}\)`| `\({Y}_{.3}\)`| `\({Y}_{.4}\)`| `\({Y}_{...}\)`|         | 
| Média      | `\(\bar{Y}_{.1}\)` | `\(\bar{Y}_{.2}\)` | `\(\bar{Y}_{.3}\)` | `\(\bar{Y}_{.4}\)` |   | `\(\bar{Y}_{..}\)` |

---

Note a estrutura de classificação da população conceitual de respostas do Quadro é representada no diagrama de Hasse:

<div class="grViz html-widget html-fill-item" id="htmlwidget-e4f04f3b6e48fb51235e" style="width:576px;height:324px;"></div>
<script type="application/json" data-for="htmlwidget-e4f04f3b6e48fb51235e">{"x":{"diagram":"\ndigraph fig6_corrected {\n  rankdir=TB;\n  node [shape=circle, style=filled, fillcolor=lightblue];\n  edge [arrowhead=none];\n\n  # Definir nós com nomes seguros\n  mu [label=\"μ\"];\n  Tratamento [label=\"T\"];\n  Unidade [label=\"U\"];\n  epsilon [label=\"ε\"];\n  \n  # Definir relações de cima para baixo\n  mu -> Tratamento;\n  mu -> Unidade;\n  Tratamento -> epsilon;\n  Unidade -> epsilon;\n  \n  {rank=same; Tratamento; Unidade;}\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>

Nesta estrutura, os fatores `Tratamento (T)` e `Unidade Experimental (U)` são **cruzados**. A identidade algébrica relacionada é:
`$$Y_{ij} = \bar{Y}_{..} + (\bar{Y}_{i.} - \bar{Y}_{..}) + (\bar{Y}_{.j} - \bar{Y}_{..}) + (Y_{ij} - \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..})$$`
---

### Da População Conceitual para a Amostra Observada
Populações conceituais de respostas **não são observáveis como um todo**.

Uma vez atribuídos os tratamentos às unidades experimentais, apenas uma resposta por u.e. é observada. Esse conjunto de respostas observadas pode ser visto como uma **amostra** da população conceitual.

**Porém, a estrutura dessa amostra não é mais a mesma da população de onde ela foi selecionada.**

Tomando a mesma situação ilustrada no Quadro anterior, numa amostra, cada unidade experimental está **embutida** num tratamento.

A **Figura** a seguir ilustra a estrutura de classificação da amostra observada no experimento:

---



**Estrutura de classificação da amostra observada no experimento**
<div class="grViz html-widget html-fill-item" id="htmlwidget-a645dc8f62e9b47ffdea" style="width:576px;height:324px;"></div>
<script type="application/json" data-for="htmlwidget-a645dc8f62e9b47ffdea">{"x":{"diagram":"digraph {\n  rankdir=TB;\n  node [shape=circle, style=filled, fillcolor=lightblue];\n  edge [arrowhead=none];\n\n  mu [label=\"μ\"];\n  epsilon [label=\"ε\"];\n  mu -> T -> U -> epsilon;\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>

A identidade algébrica relacionada a uma observação do experimento é:
`$$y_{ir} = \bar{y}_{..} + (\bar{y}_{i.} - \bar{y}_{..}) + (y_{ir} - \bar{y}_{i.})$$`

---
##  Modelos Lineares Classificatórios

Para ilustrar a utilidade dos modelos lineares no contexto de planejamento de experimentos, considere ainda a situação correspondente ao Quadro do experimento que estamos estudando como exemplo. Imaginemos que o experimento foi realizado, fornecendo os seguintes dados:

| Tratamento | Resposta     |
|------------|--------------|
| A (1)      | `\(y_{11}\)` (15.2) |
| A (1)      | `\(y_{12}\)` (14.3) |
| B (2)      | `\(y_{21}\)` (13.5) |
| B (2)      | `\(y_{22}\)` (14.5) |

Um modelo linear apropriado para essa situação experimental pode ser escrito da seguinte forma:
`$$\begin{aligned}
y_{ir} &amp;= \mu_i + \epsilon_{ir} \\
&amp;= \mu + \tau_i + \epsilon_{ir} \quad (1)
\end{aligned}$$`
---

onde:
- `\(y_{ir}\)` é a resposta observada na r-ésima réplica do tratamento `\(i=1,2; r=1,2; (i=1=A. i=2=B)\)`;
- `\(\mu\)` representa a média geral das observações;
- `\(\tau_i\)` é o efeito (fixo) do tratamento `\(i\)`;
- `\(\epsilon_{ir}\)` é o termo aleatório que representa o resíduo associado à réplica `\(r\)` do tratamento `\(i\)`;

Ajustar o modelo (1) aos dados significa achar uma "melhor" solução para o sistema de equações lineares em `\(\mu\)`, `\(\tau_1\)` e `\(\tau_2\)`:
$$ \mu + \tau_1 \doteq 15.2 $$
$$ \mu + \tau_1 \doteq 14.3 $$
$$ \mu + \tau_2 \doteq 13.5 $$
$$ \mu + \tau_2 \doteq 14.5 $$
Este sistema pode ser escrito na forma `\(X\beta \doteq y\)`:
`$$\begin{pmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 1 \end{pmatrix} \begin{pmatrix} \mu \\ \tau_1 \\ \tau_2 \end{pmatrix} \doteq \begin{pmatrix} 15.2 \\ 14.3 \\ 13.5 \\ 14.5 \end{pmatrix} \quad (2)$$`
---
- `\(X\)` é a **matriz do modelo**. Sua forma é conhecida e, no contexto deste curso, tem **posto incompleto**.
- `\(\beta\)` é o vetor de parâmetros fixos (porém desconhecidos).
- `\(y\)` é o vetor de observações (medidas) da variável resposta.

Em termos matriciais, o modelo (1) corresponde a `$$y = X\beta + \epsilon \quad (3),$$`
em que `\(\epsilon = (\epsilon_{11},\epsilon_{12},\epsilon_{21},\epsilon_{22} )^\top\)` é o vetor aleatório de erros. 

##  O Modelo Linear de Gauss-Markov

O modelo linear de Gauss-Markov é definido como:
`$$y = X\beta + \epsilon, \quad (4)$$`
onde as premissas sobre o vetor aleatório de resíduos `\(\epsilon\)` são:
1.  `\(E(\epsilon) = 0\)` (A média dos erros é zero).
2.  `\(Var(\epsilon) = \sigma^2 I\)` (Os erros são não correlacionados e possuem variância constante, `\(\sigma^2\)`).

Estas são as premissas clássicas que garantem as ótimas propriedades do estimador de mínimos quadrados.
---

## O ajuste de quadrados mínimos


**Critério:** Minimizar a soma dos quadrados dos resíduos.

O objetivo é achar o valor de `\(\beta\)`, diga-se `b`, que torna mínima a função distância `\(D\)` dada por:
`$$D(y, X\beta) = (y - X\beta)^\top(y - X\beta)$$`
Resolvendo-se analiticamente, tem-se:
$$\frac{dD(y, X\beta)}{d\beta} = 2(X^\top X \beta - X^\top y) = 0 $$
O resultado anterior indica que qualquer vetor `b` que satisfaça o conjunto das **Equações Normais** é uma solução de quadrados mínimos:
$$(X^\top X) b = X^\top y \quad (5) $$
**Obs:** Como a matriz `\(X\)` tem posto incompleto `\(\implies\)` `\(X^\top X\)` é singular `\(\implies\)` Existem infinitas soluções `b` para as equações normais (5). **Como achar uma forma geral para `b`?**
---

### Definições: Inversas e Projeção

**Definição 1: Inversa Generalizada:** Uma matriz `\(G\)` é uma inversa generalizada de `\(A\)` se `\(AGA=A\)`. Notação: `\(G=A^-\)`. Elas geralmente não são únicas.

**Comentários:**

- Inversas generalizadas, em geral, não são únicas.
- Se `\(A\)` for uma matriz m◊m não singular, `\(G = A^{-1}\)` será a única inversa generalizada de `\(A\)`.

**Definição 2: Inversa de Moore-Penrose:** Para qualquer matriz `\(A\)` , existe uma
única matriz `\(M\)` , chamada de inversa generalizada de Moore-Penrose, que
satisfaz as seguintes condições:
1. `\(AMA = A\)`
2. `\(MAM = M\)`
3. `\(AM\)` é simétrica
4. `\(MA\)` é simétrica

**Definição 3: Matriz de Projeção:** A matriz `\(P_A = A(A^\top A)^- A^\top\)` projeta vetores ortogonalmente no espaço gerado pelas colunas de `\(A\)`.
---

**Propriedades de `\(P_A\)`:**

i) É **invariante** à escolha da inversa generalizada `\((A^\top A)^-\)`.

ii) É simétrica (`\(P_A^\top = P_A\)`).

iii) É idempotente (`\(P_A  P_A = P_A\)`).

iv) `\(P_A A = A\)`.

v) `\(P_A(I-P_A) = 0\)`.

vi) Particionando a matriz `\(A\)` como `\(A=[A_1| A_2| ...| A_k]\)`,
`\(P_A A_j = A_j.\)` 

vii) `\((I - P_A)\)` é uma matriz de projeção que projeta
vetores no complemento ortogonal de `\(A.\)` 

---
### Solução das Equações Normais e Propriedades

**Resultado ** `$$b = (X^\top X)^- X^\top y$$` é solução das equações normais.

**Prova:**
Substituindo `\(b\)` em `\(X^\top X b\)`:
`\(X^\top X b = X^\top X [(X^\top X)^- X^\top y]\)`
Usando a propriedade da matriz de projeção `\(P_X = X(X^\top X)^- X^\top\)`, temos que `\(X^\top P_X = X^\top\)`.
Então: `\(X^\top X b = (X^\top P_X) X^\top y = X^T y\)`. C.Q.D.

**Ponto Chave:** Mesmo que `b` não seja único, o vetor de valores preditos `\(\hat{y}\)` **é único** porque a matriz de projeção `\(P_X\)` é única.

**Resultado** Sob as condições de Gauss-Markov:

a) `\(E(y) = X\beta\)`

b) `\(\hat{y} = P_X y = Xb\)` é um estimador centrado (não-viesado) para `\(E(y) = X\beta\)`.

c) `\(Var(\hat{y}) = \sigma^2 P_X = \sigma^2 X(X^\top X)^- X^\top\)`
---

### Prova da Propriedade (a): `\(E(y) = X\beta\)`

Esta prova decorre diretamente das premissas do modelo.

1.  **Modelo:** Partimos do modelo linear:
    `\(y = X\beta + \epsilon\)`

2.  **Aplicar a Esperança:** Aplicamos o operador de esperança a ambos os lados da equação:
    `\(E(y) = E(X\beta + \epsilon)\)`

3.  **Linearidade da Esperança:** A esperança da soma é a soma das esperanças:
    `\(E(y) = E(X\beta) + E(\epsilon)\)`

4.  **Propriedades da Esperança:**
    - O termo `\(X\beta\)` é um vetor de constantes (parâmetros fixos), então `\(E(X\beta) = X\beta\)`  e pela premissa do modelo de Gauss-Markov, `\(E(\epsilon) = 0\)`.

5.  **Substituição:** Substituindo estes resultados na equação:
    $$ E(y) = X\beta + 0 $$

6.  **Conclusão:** `\(E(y) = X\beta\)`
    Isso mostra que o valor esperado do vetor de observações é o componente determinístico do modelo. **C.Q.D.**

---

### Prova da Propriedade (b): `\(\hat{y}\)` é um estimador centrado (não-viesado)

Queremos provar que `\(E(\hat{y}) = X\beta\)`.

1.  **Definição do Estimador:** O vetor de valores preditos é definido como `\(\hat{y} = P_X y\)`.
    `\(\hat{y} = X(X^\top X)^- X^\top y\)`

2.  **Aplicar a Esperança:** `\(E(\hat{y}) = E(P_X y)\)`

3.  **Propriedades da Esperança:** A matriz de projeção `\(P_X\)` é uma matriz de constantes, então podemos colocá-la para fora da esperança:
    `\(E(\hat{y}) = P_X E(y)\)`

4.  **Usar Resultado Anterior:** Pela propriedade (a), já provamos que `\(E(y) = X\beta\)`. Substituindo:
    `\(E(\hat{y}) = P_X (X\beta)\)`

5.  **Propriedade da Matriz de Projeção:** Uma das propriedades fundamentais da matriz de projeção no espaço de colunas de X é que `\(P_X X = X\)`.
    
6.  **Conclusão:**
    `\(E(\hat{y}) = X\beta\)`
    Como o valor esperado do estimador `\(\hat{y}\)` é igual ao parâmetro que ele visa estimar `\(E(y)\)`, concluímos que `\(\hat{y}\)` é um estimador **centrado (não-viesado)**. **C.Q.D.**

---

### Prova da Propriedade (c): `\(Var(\hat{y}) = \sigma^2 P_X\)`

Queremos provar que a matriz de covariâncias do vetor de preditos é `\(\sigma^2 P_X\)`.

1.  **Definição do Estimador:** Novamente, partimos de `\(\hat{y} = P_X y\)`.

2.  **Aplicar a Variância:** Aplicamos o operador de variância-covariância:
    `\(Var(\hat{y}) = Var(P_X y)\)`

3.  **Propriedade da Matriz de Covariâncias:** Para uma matriz de constantes `A` e um vetor aleatório `y`, a propriedade é `\(Var(Ay) = A \cdot Var(y) \cdot A^T\)`. Como `\(Var(\hat{y}) = P_X \cdot Var(y) \cdot P_X^\top\)`

4.  **Variância de y:** Pela premissa do modelo de Gauss-Markov, `\(Var(y) = Var(X\beta + \epsilon) = Var(\epsilon) = \sigma^2 I\)`. Substituindo:
    `\(Var(\hat{y}) = P_X (\sigma^2 I) P_X^\top\)`

5.  **Simplificação:** Como `\(\sigma^2\)` é um escalar, podemos movê-lo para a frente:
    `\(Var(\hat{y}) = \sigma^2 P_X I P_X^\top = \sigma^2 P_X P_X^\top\)`

6.  **Propriedades da Matriz de Projeção:** Como a matriz de projeção `\(P_X\)` é simétrica, então `\(P_X^\top = P_X\)` e idempotente, então `\(P_X P_X = P_X\)`.

7.  **Substituição e Conclusão:**
    `\(Var(\hat{y}) = \sigma^2 P_X P_X = \sigma^2 P_X,\)` logo `\(Var(\hat{y}) = \sigma^2 X(X^\top X)^- X^\top\)`
    Isso completa a prova. A matriz de covariâncias do vetor de preditos é a matriz de projeção `\(P_X\)` multiplicada pela variância do erro, `\(\sigma^2\)`. **C.Q.D.**


---
### Como Gerar uma Inversa Generalizada


**Problema:** Encontrar uma inversa generalizada para `\(X^\top X = \begin{pmatrix} 4 &amp; 2 &amp; 2 \\ 2 &amp; 2 &amp; 0 \\ 2 &amp; 0 &amp; 2 \end{pmatrix}\)`. Note que posto(`\(X^TX\)`) = 2.

**Algoritmo 1:**
1.  Encontre uma submatriz `\(2\times2\)` de posto completo. Por exemoplo, a
submatriz no canto inferior,
mais à direita, dada por `\(S = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)`.
2.  Obtenha a matriz transposta da
inversa da submatriz escolhida. No caso do exemplo dado: `\((S^{-1})^\top = (\begin{pmatrix} 1/2 &amp; 0 \\ 0 &amp; 1/2 \end{pmatrix})^\top = \begin{pmatrix} 1/2 &amp; 0 \\ 0 &amp; 1/2 \end{pmatrix}\)`.
3.  Coloque no lugar original da matriz `\(G\)` e preencha o resto com zeros: `\(G = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 0 \\ 0 &amp; 0 &amp; 1/2 \end{pmatrix}\)`.
---
4. Obtenha a transposta da
matriz resultante do passo
3 e você terá uma inversa
generalizada: 
`\(G^- = (X^\top X)^- = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 0 \\ 0 &amp; 0 &amp; 1/2 \end{pmatrix}\)`.


### Algoritmo 2: Inversa Generalizada via SVD
O segundo algoritmo para achar uma inversa generalizada é mais geral e teoricamente mais elegante. Ele se baseia na **Decomposição em Valores Singulares (SVD)**.

---
Para achar uma inversa generalizada de uma matriz `\(A\)` de dimensão `\(m \times n\)` e posto `\(r\)`, siga os passos seguintes:

**Passo 1: Decomponha a matriz `\(A\)` em seus valores singulares (SVD).**
A SVD de `\(A\)` é a fatoração: `\(A = U \Sigma V^\top.\)`  Podemos usar também uma variação relacionada chamada decomposição em posto completo, que é escrita como:
`$$PAQ = \begin{pmatrix} D_{r \times r} &amp; 0 \\ 0 &amp; 0 \end{pmatrix}$$`
Onde:
- `\(P\)` é uma matriz ortogonal `\(m \times m\)`.
- `\(Q\)` é uma matriz ortogonal `\(n \times n\)`.
- `\(D\)` é uma matriz diagonal `\(r \times r\)` contendo os `\(r\)` valores singulares positivos de `\(A\)`.

---


**Passo 2: Construa a inversa generalizada.**
Faça:$$G = Q \begin{pmatrix} D^{-1} &amp; E_1 \\ E_2 &amp; E_3 \end{pmatrix} P$$
Onde `\(D^{-1}\)` é simplesmente a matriz diagonal com o inverso de cada valor singular, e `\(E_1, E_2, E_3\)` são matrizes de dimensões apropriadas compostas por elementos arbitrários (quaisquer números).

**Interpretação:**
- A parte `\(D^{-1}\)` "inverte" a parte principal da matriz `\(A\)`.
-- As matrizes `\(E_1, E_2, E_3\)` correspondem ao **espaço nulo** da matriz A. Como existem infinitas maneiras de mapear o espaço nulo, a escolha de `\(E_1, E_2, E_3\)` é arbitrária, o que gera as **infinitas inversas generalizadas**.

---
### A Conexão com a Inversa de Moore-Penrose

A inversa generalizada mais "natural" e comumente usada é a **inversa de Moore-Penrose**. Ela é obtida a partir do Algoritmo 2 com a escolha mais simples possível para as matrizes arbitrárias.

Em particular, escolhendo:
$$ E_1 = 0, \quad E_2 = 0, \quad E_3 = 0 $$
temos a inversa generalizada de Moore-Penrose, frequentemente denotada por `\(A^+\)`:

$$ A^+ = Q \begin{pmatrix} D^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix} P $$

Esta é a inversa generalizada que a função `ginv()` do pacote `MASS` calcula no `R`.

---
### Exemplo Prático em R: Usando SVD para encontrar `\(A^+\)`

Vamos encontrar a inversa de Moore-Penrose para a matriz `\(A = X^T X\)` do nosso exemplo, usando a SVD.

`\(A = \begin{pmatrix} 4 &amp; 2 &amp; 2 \\ 2 &amp; 2 &amp; 0 \\ 2 &amp; 0 &amp; 2 \end{pmatrix}\)`


``` r
# 1. Nossa matriz A (que é XTX)
A &lt;- matrix(c(4, 2, 2, 2, 2, 0, 2, 0, 2), ncol=3, byrow=TRUE)

# 2. Realizar a Decomposição em Valores Singulares (SVD) em R
svd_A &lt;- svd(A)

# A SVD em R nos dá U, V e um vetor d com os valores singulares
U &lt;- svd_A$u
V &lt;- svd_A$v
d &lt;- svd_A$d

cat("Valores singulares (d):\n")
```

```
## Valores singulares (d):
```

``` r
print(d)
```

```
## [1] 6 2 0
```
---


``` r
# O posto é 2, pois o terceiro valor singular é praticamente zero.
# Vamos inverter apenas os valores singulares não-nulos.
D_inv_vec &lt;- 1 / d[d &gt; 1e-10]
D_inv &lt;- diag(c(D_inv_vec, 0)) # Criar a matriz D^{-1} com o zero no final

# 3. Construir a inversa de Moore-Penrose: A+ = V * D_inv * t(U)
A_plus &lt;- V %*% D_inv %*% t(U)

cat("\nInversa de Moore-Penrose calculada via SVD:\n")
```

```
## 
## Inversa de Moore-Penrose calculada via SVD:
```

``` r
print(A_plus)
```

```
##            [,1]        [,2]        [,3]
## [1,] 0.11111111  0.05555556  0.05555556
## [2,] 0.05555556  0.27777778 -0.22222222
## [3,] 0.05555556 -0.22222222  0.27777778
```

``` r
# 4. Comparar com o resultado da função ginv() do pacote MASS
cat("\nInversa calculada pela função ginv():\n")
```

```
## 
## Inversa calculada pela função ginv():
```

``` r
print(ginv(A))
```

```
##            [,1]        [,2]        [,3]
## [1,] 0.11111111  0.05555556  0.05555556
## [2,] 0.05555556  0.27777778 -0.22222222
## [3,] 0.05555556 -0.22222222  0.27777778
```
**Conclusão:** Os resultados são idênticos. O Algoritmo 2, quando usamos zeros para as matrizes arbitrárias, nos dá exatamente a inversa de Moore-Penrose. Ele fornece o fundamento teórico para o cálculo que funções como `ginv()` realizam internamente.

---


### EXERCÍCIOS EM CLASSE 

**1. Achar outra inversa generalizada de `\(X^\top X\)` usando o algoritmo 1.**
1. Escolhemos outra submatriz de posto completo: `\(S = \begin{pmatrix} 4 &amp; 2 \\ 2 &amp; 2 \end{pmatrix}\)`.
2. Sua inversa é `\(S^{-1} = \begin{pmatrix} 0.5 &amp; -0.5 \\ -0.5 &amp; 1 \end{pmatrix}\)`.
3. Colocando no lugar original e preenchendo com zeros, temos uma inversa generalizada: `$$G_1 = \begin{pmatrix} 0.5 &amp; -0.5 &amp; 0 \\ -0.5 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}$$`

**2. Obter a estimativa de `\(\beta\)` com `\(G_1\)` e os dados.**

``` r
X &lt;- matrix(c(1,1,0, 1,1,0, 1,0,1, 1,0,1), ncol=3, byrow=TRUE)
y_vec &lt;- c(15.2, 14.3, 13.5, 14.5)
G1 &lt;- matrix(c(0.5, -0.5, 0, -0.5, 1, 0, 0, 0, 0), ncol=3, byrow=TRUE)
b_estimado_g1 &lt;- G1 %*% t(X) %*% y_vec
rownames(b_estimado_g1) &lt;- c("μ", "τ₁", "τ₂")
```
---


``` r
print(b_estimado_g1)
```

```
##     [,1]
## μ  14.00
## τ₁  0.75
## τ₂  0.00
```

### Definição 3: Estimabilidade

Uma combinação linear `\(l^T \beta\)` é dita **estimável** se existe um estimador linear e não-viesado para ela (`\(E(a^T y) = l^T\beta\)`). Isso significa que sua estimativa `\(l^T b\)` é **invariante** à escolha da inversa generalizada.

**Exemplo:** `\(\mu + \tau_1\)` é estimável.
---

**Exercício: Estimar as quantidades `\(\mu + \tau_1\)`, `\(\mu + \tau_2\)` e `\(\tau_1 - \tau_2\)` com o `b` encontrado.**

``` r
L &lt;- matrix(c(
  1, 1, 0,  # l₁ para μ + τ₁
  1, 0, 1,  # l₂ para μ + τ₂
  0, 1, -1  # l₃ para τ₁ - τ₂
), ncol=3, byrow=TRUE)

estimativas_g1 &lt;- L %*% b_estimado_g1
rownames(estimativas_g1) &lt;- c("μ + τ₁", "μ + τ₂", "τ₁ - τ₂"); print(estimativas_g1)
```

```
##          [,1]
## μ + τ₁  14.75
## μ + τ₂  14.00
## τ₁ - τ₂  0.75
```
**Conclusão:** Os valores para as funções estimáveis (14.75, 14.0, 0.75) são únicos. Seus colegas chegarão aos mesmos resultados, mesmo que seus vetores `b` sejam diferentes.

---
## 3.7-3.9: Teoremas Fundamentais
(Páginas 114-120)

**Teorema de Gauss-Markov (Teorema 3.1):** Sob o modelo de Gauss-Markov, o estimador de mínimos quadrados de uma função estimável `\(l^T\beta\)` é o **BLUE** (Best Linear Unbiased Estimator - Melhor Estimador Linear Não-Viesado).

**Modelo de Aitken (Seção 3.9):** Generaliza o modelo para casos onde a matriz de covariância dos erros não é diagonal e/ou homocedástica: `\(Var(\epsilon) = \sigma^2 H\)`, com `\(H \neq I\)`.

**Teorema 3.2 (Aitken):** Sob o modelo de Aitken, o estimador BLUE é o de **Mínimos Quadrados Generalizados (GLS)**, também chamado de *quadrados mínimos ponderados*:
$$ b_{GLS} = (X^T H^{-1} X)^{-} X^T H^{-1} y $$

**Teorema 3.3 (Zyskind):** Responde à pergunta: quando o estimador de Mínimos Quadrados Ordinários (OLS) ainda é BLUE, mesmo sob o modelo de Aitken? A resposta é afirmativa se existir uma matriz Q tal que `\(VX=XQ\)`, onde `\(V = \sigma^2H\)`.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
