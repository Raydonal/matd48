---
title: "Fundamentos de Análise para Planejamento Experimental"
subtitle: "Noções de Modelos Lineares para Planejamento experimental"
author: "Raydonal Ospina"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "custom-styles.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
bibliography: refs.bib
csl: apa.csl    
link-citations: true
---


```{r setup, include=FALSE}
# Opções globais do Knitr
options(html.tag.transform.rmarkdown = function(x) x)
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', message = FALSE, warning = FALSE, fig.width=8, fig.height=4.5)

# Carregar pacotes necessários
library(ggplot2)
library(dplyr)
library(MASS) # Para a função ginv()
library(DiagrammeR)
```

class: center, middle, inverse

# 1. Fundamentos e Estrutura de Modelos Lineares

(Seções 3.1 a 3.5)

---

## Modelos Lineares Estatísticos

Modelos lineares estatísticos são a espinha dorsal da análise de dados experimentais. Eles nos permitem modelar a relação entre uma variável de resposta e um ou mais fatores ou variáveis preditoras.

.pull-left[
**Estrutura Matemática Geral:**

$$ Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \epsilon $$

- **Componente Determinístico:** $\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p$. Descreve a relação *média* entre Y e os X's.
- **Componente Estocástico:** $\epsilon$. Representa a variabilidade aleatória não explicada pelo modelo, incluindo erros de medição e variação biológica/natural.
]
.pull-right[
```{r, echo=FALSE}
set.seed(42)
x <- 1:20
y <- 2.5 * x + rnorm(20, 0, 10)
qplot(x, y, geom = c("point", "smooth"), method = "lm", se = FALSE) + 
  theme_minimal(base_size = 14) + labs(title="Exemplo Visual de um Modelo Linear", x="Preditor (X)", y="Resposta (Y)")
```
]

---

## 3.1 Relações entre Fatores

Em um experimento, um **fator** particiona a população em grupos (seus **níveis**). A complexidade e a validade de um modelo dependem crucialmente de como os múltiplos fatores se relacionam.

--

As possíveis relações são:

1.  **Embutimento (Aninhamento):** Uma relação hierárququica.
2.  **Cruzamento:** Uma relação fatorial completa.
3.  **Confundimento:** Uma falha de delineamento onde os efeitos são indistinguíveis.

Compreender essas relações é o primeiro passo para construir um modelo estatístico correto.

---

## Fatores Embutidos (Aninhados)

Um fator `B` é dito estar **embutido** (ou aninhado) em um fator `A` se cada nível do fator `B` ocorre em combinação com apenas **um** nível do fator `A`.

### Exemplo Simulado: `Turma` aninhada em `Escola`

```{r}
set.seed(321)
dados_aninhados <- data.frame(
  Escola = factor(rep(c("Escola A", "Escola B", "Escola C"), each = 10)),
  # A Turma 'T1' da Escola A é diferente da Turma 'T1' da Escola B
  Turma = factor(rep(c("A.T1", "A.T2", "B.T1", "B.T2", "C.T1", "C.T2"), each = 5)),
  Nota = round(c(rnorm(5, 85, 4), rnorm(5, 82, 4), # Turmas da Escola A
                 rnorm(5, 75, 5), rnorm(5, 78, 5), # Turmas da Escola B
                 rnorm(5, 80, 3), rnorm(5, 79, 3)), 1) # Turmas da Escola C
)
```

**Implicação no Modelo:** O efeito de `Turma` deve ser modelado *dentro* do efeito de `Escola`. Um modelo aditivo `Nota ~ Escola + Turma` estaria **incorreto**. O modelo correto em R é `Nota ~ Escola / Turma`.

---

### Visualizando Dados Aninhados

Um boxplot revela a estrutura hierárquica. Vemos a variação entre turmas *dentro* de cada escola, e a variação entre as escolas.

```{r, fig.height=4}
ggplot(dados_aninhados, aes(x = Turma, y = Nota, fill = Escola)) +
  geom_boxplot() +labs(title = "Visualização de Dados Aninhados",
       subtitle = "Variação de Notas por Turma dentro de cada Escola",
       x = "Turmas (identidade única dentro de cada escola)",
       y = "Nota") + theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Interpretação Gráfica:** A comparação principal é entre Escolas. A comparação entre "A.T1" e "B.T1" não é direta, pois são populações de alunos diferentes em contextos diferentes.

---

## Fatores Cruzados

Dois fatores `A` e `B` são ditos **cruzados** quando cada nível de `A` ocorre em combinação com **todos** os níveis de `B`.

### Exemplo Simulado: `Droga` x `Dieta`

```{r}
set.seed(456)
dados_cruzados <- data.frame(
  Droga = factor(rep(c("Droga A", "Droga B"), each = 10)),
  Dieta = factor(rep(c("Normal", "Hipercalórica"), times = 2, each = 5)),
  Resposta = round(c(rnorm(5, 50, 8), rnorm(5, 52, 8), # Droga A
                     rnorm(5, 60, 8), rnorm(5, 40, 8)), 1) # Droga B
)
```
**Implicação no Modelo:** Permite estimar **efeitos principais** e a **interação**. O modelo em R é `Resposta ~ Droga * Dieta`.

---

### Visualizando Dados Cruzados: Efeitos Principais

Boxplots são ótimos para visualizar os efeitos principais de cada fator. Podemos visualizar o efeito principal de cada fator separadamente
.pull-left[
```{r, fig.height=4.5}
ggplot(dados_cruzados, aes(x = Droga, y = Resposta, fill = Droga)) +
  geom_boxplot() + labs(title = "Efeito Principal da Droga") +
  theme_minimal(base_size = 12) + theme(legend.position = "none")
```
]
.pull-right[
```{r, fig.height=4.5}
ggplot(dados_cruzados, aes(x = Dieta, y = Resposta, fill = Dieta)) +
  geom_boxplot() + labs(title = "Efeito Principal da Dieta") +
  theme_minimal(base_size = 12) + theme(legend.position = "none")
```
]

---

### Visualizando Dados Cruzados: Interação

Um gráfico de interação (ou de perfis) é a melhor ferramenta para visualizar se os efeitos são aditivos ou se interagem. Isso mostra o efeito médio de um fator, ignorando (ou agregando sobre) os níveis do outro. Corresponde conceitualmente às linhas "Droga" e "Dieta" na tabela de ANOVA.


```{r, fig.height=4}
ggplot(dados_cruzados, aes(x = Droga, y = Resposta, color = Dieta, group = Dieta)) +
  stat_summary(fun = mean, geom = "point", size = 4) +
  stat_summary(fun = mean, geom = "line", linewidth = 1.5) +
  labs(title = "Gráfico de Interação: Droga x Dieta",
       subtitle = "Linhas não paralelas sugerem interação",
       y = "Resposta Média") + theme_minimal(base_size = 14)
```

**Interpretação Gráfica:** As linhas se cruzam! O efeito da Droga B é positivo na dieta Normal, mas negativo na Hipercalórica. Isso é uma **interação qualitativa** forte. Um modelo sem o termo de interação falharia em capturar essa dinâmica crucial.

---

class: center, middle, inverse

## 3.2 e 3.3: A Linguagem dos Diagramas de Hasse: Da Estrutura ao Modelo

---

### Aprofundando: O que é um Diagrama de Hasse?

Um Diagrama de Hasse é a representação gráfica de um **conjunto parcialmente ordenado (poset)**. No nosso contexto:
- **O conjunto:** É composto pelos fatores do modelo, mais a média geral ($\mu$) e o termo de erro residual ($\epsilon$).
- **A relação de ordem ($\preceq$):** É definida pela **relação de aninhamento**. Dizemos que $B \preceq A$ ("B precede ou é igual a A") se o fator B está aninhado no (ou é contido pelo) fator A.

O diagrama simplifica a visualização desta ordem, mostrando apenas as relações de "cobertura" (se $B \preceq A$ e não existe um C tal que $B \preceq C \preceq A$, então desenhamos uma linha de B para A). A direção da hierarquia é implícita (de baixo para cima).

### Por que isso é fundamental?
A estrutura de ordem parcial do diagrama dita diretamente:
1.  Quais termos são permitidos no modelo estatístico.
2.  Quais médias são "admissíveis" para cálculo e interpretação.
3.  Como a Soma de Quadrados deve ser particionada na ANOVA.

---

### A Gramática Visual dos Diagramas de Hasse

Para usar o diagrama, precisamos entender sua linguagem.

.pull-left[
**1. Aninhamento (Hierarquia)**
- **Relação:** Fator `B` está aninhado em `A` ($A \supset B$).
- **Visual:** Uma linha vertical direta conectando `B` (embaixo) a `A` (em cima).
```{r, echo=FALSE,fig.height=2.5}
grViz("digraph { node[shape=circle]; edge[arrowhead=none]; B -> A }")
```
- **Interpretação:** O efeito de `B` só pode ser avaliado *dentro* dos níveis de `A`.
]
.pull-right[
**2. Cruzamento (Fatorial)**
- **Relação:** Fatores `A` e `B` são cruzados ($A \times B$).
- **Visual:** Estrutura paralela. Ambos partem de um nó superior comum e chegam a um nó inferior comum.
```{r, echo=FALSE, fig.height=2.5}
grViz("digraph { node[shape=circle]; edge[arrowhead=none]; E -> {A B} -> M }")
```
- **Interpretação:** Os efeitos de `A`, `B` e da interação `A:B` podem ser estimados.
]

---

### Do Diagrama de Hasse para a Fórmula do Modelo em R

**Esta é a conexão mais importante:** O diagrama de Hasse é um mapa visual que se traduz diretamente na sintaxe de fórmulas do R.

.pull-left[
**1. Aninhamento (Hierarquia)**

O diagrama mostra `B` aninhado em `A`.

```{r, echo=FALSE, fig.height=2.5}
grViz("
digraph hierarchy {
  rankdir=TB;
  node [shape=circle, style=filled, fillcolor=lightblue];
  edge [arrowhead=none];
  B -> A;
}
")
```

**Tradução para R:** O operador de aninhamento é `/`.
O modelo para o efeito de `B` aninhado em `A` é:
`~ A / B`
que é expandido pelo R para:
`~ A + A:B`
]
.pull-right[
**2. Cruzamento (Fatorial)**

O diagrama mostra `A` e `B` cruzados.

```{r, echo=FALSE, fig.height=2.5}
grViz("
digraph factorial {
  rankdir=TB;
  node [shape=circle, style=filled, fillcolor=lightblue];
  edge [arrowhead=none];
  
  # Nós
  mu [label='μ']
  epsilon [label='ε']

  # Relações
  epsilon -> {A, B} -> mu;
}
")
```

**Tradução para R:** O operador de cruzamento é `*`.
O modelo para `A` e `B` cruzados é:
`~ A * B`
que é expandido pelo R para:
`~ A + B + A:B`
]

**Portanto, ao desenhar o diagrama de Hasse para o seu delineamento experimental, você está, na verdade, especificando a fórmula correta do seu modelo estatístico.**

---

### Exemplo Prático: Delineamento em Blocos Completos Aleatorizados (RCBD)

Este é um exemplo clássico que ilustra o poder dos diagramas para escolher o modelo correto.

**Cenário Experimental:**
- Queremos testar o efeito de 3 **Tratamentos** (A, B, C) (nosso fator de interesse).
- Suspeitamos que há um gradiente de fertilidade no campo (uma fonte de variabilidade que não nos interessa, mas que pode mascarar o efeito do tratamento).
- Para controlar essa variabilidade, dividimos o campo em 4 **Blocos**. Cada bloco é uma área que consideramos homogênea internamente, mas pode ser diferente de outros blocos.
- Em cada bloco, aplicamos *todos* os 3 tratamentos a 3 unidades experimentais, de forma aleatória.
---

**Análise das Relações:**
- O fator de interesse é `Tratamento`.
- O fator de controle (ou "ruído") é `Bloco`.
- Como cada tratamento aparece em cada bloco, os fatores `Tratamento` e `Bloco` são **cruzados**.
- A interação `Bloco:Tratamento` existe? Sim, mas em um RCBD sem réplicas, ela está **confundida** com o erro experimental residual. A unidade experimental ($\epsilon$) é, na verdade, a combinação de um Bloco com um Tratamento.

---

### Diagrama de Hasse e Modelo para o Delineamento em Blocos (RCBD)


**Construindo o Diagrama:**
1.  A unidade experimental ($\epsilon$), que representa a variação `Bloco:Tratamento`, está no nível mais baixo.
2.  Acima dela, estão os fatores que a definem: `Bloco` e `Tratamento`.
3.  Como `Bloco` e `Tratamento` são cruzados, eles formam uma estrutura paralela.
4.  Ambos estão abaixo da média geral ($\mu$).

```{r, echo=FALSE, fig.height=2.5}
grViz("
digraph rcbd {
  node [shape=box, style=rounded]
  edge [arrowhead=none]
  
  # Nós
  mu [label='μ']
  bloco [label='Bloco']
  trat [label='Tratamento']
  epsilon [label='ε \n(Bloco:Tratamento)', shape=ellipse]

  # Relações
  epsilon -> {bloco, trat} -> mu
}
")
```
---

**Tradução para Modelo Estatístico:**

O diagrama mostra claramente dois fatores cruzados. Em um delineamento em blocos, geralmente não estamos interessados na interação bloco-tratamento (assumimos que ela não existe ou é pequena). Nosso objetivo é **remover a variabilidade devida aos Blocos** para conseguir enxergar o efeito dos Tratamentos com mais clareza.

**Fórmula do Modelo em R:**
O diagrama nos diz que `Bloco` e `Tratamento` são fatores cruzados. Como queremos estimar seus efeitos de forma aditiva (sem a interação), a fórmula correta é:

`Resposta ~ Bloco + Tratamento`

**O que este modelo faz?**
- `Tratamento`: Estima e testa o efeito do nosso fator de interesse.
- `Bloco`: Estima a variabilidade *entre* os blocos e a **remove** da soma de quadrados do resíduo.

Isso torna o termo de erro (`QM_Resíduo`) menor, o que aumenta o poder do nosso teste F para detectar diferenças entre os tratamentos. O diagrama de Hasse nos levou diretamente a este modelo, que é o padrão para a análise de um RCBD.

**Se estivéssemos interessados na interação, o modelo seria:**
`Resposta ~ Bloco * Tratamento` (mas não poderíamos estimar o erro residual separadamente sem réplicas).


---

### O Diagrama de Hasse como Ferramenta de Diagnóstico

Podemos usar o diagrama *antes* de realizar o experimento para verificar se o nosso delineamento é sólido.

**Cenário do Confundimento:** `Método de Ensino` confundido com `Professor`.
- O Prof. Silva SEMPRE usa o Método X.
- A Profa. Santos SEMPRE usa o Método Y.

**Análise Estrutural:**
Qualquer diferença observada entre os grupos pode ser devida ao `Professor` OU ao `Método`. Eles são indistinguíveis. Na linguagem de conjuntos e ordem parcial, eles são o mesmo elemento.

**Diagrama de Hasse do Delineamento Falho:**
```{r, echo=FALSE, fig.height=2.5}
grViz("
digraph confounded {
  node [shape=circle, style=filled, fillcolor=lightcoral]
  edge [arrowhead=none]
  
  # Nós confundidos em um só
  conf [label='(Professor, Método)']
  
  # Relações
  epsilon -> conf -> mu
  
  # Rótulos
  mu [label = 'μ']; epsilon [label = 'ε']
}
")
```

**Diagnóstico:** O diagrama mostra visualmente que não há como separar os efeitos. Se tivéssemos desenhado este diagrama durante a fase de planejamento, a falha seria óbvia e o experimento poderia ser corrigido (ex: fazendo ambos os professores usarem ambos os métodos, o que resultaria em um delineamento cruzado).
---
class: center, middle, inverse

## 3.6 a 3.8: Ajuste, Estimação e Teoria

---

### O Modelo em Forma Matricial: $y = X\beta + \epsilon$
A matriz $X$ é a **representação matemática do seu delineamento experimental**.

### O Princípio de Mínimos Quadrados e as Equações Normais
O objetivo é encontrar $b$ que minimiza $S(b) = (y - Xb)^T(y - Xb)$.
A solução é encontrada derivando $S(b)$ em relação a $b$ e igualando a zero:
$$ \frac{\partial S(b)}{\partial b} = -2X^T(y - Xb) = 0 \implies X^T y - X^T X b = 0 $$
O que nos leva às **Equações Normais**:
$$ (X^T X) b = X^T y $$

---

### Singularidade, Inversa Generalizada e Estimabilidade

A matriz $X^T X$ é singular em modelos fatoriais devido à **sobreparametrização**. A solução $b$ não é única.

**Solução:** $b = (X^T X)^- X^T y$, onde $(X^T X)^-$ é *qualquer* **inversa generalizada**.

**Implicação Profunda: Estimabilidade.**

Embora $b$ não seja único, o vetor de valores preditos $\hat{y} = Xb = X(X^T X)^- X^T y = P_X y$ **é único**, pois a matriz de projeção $P_X$ é invariante à escolha da inversa generalizada.

Uma função $l^T \beta$ é **estimável** se ela pode ser escrita como uma combinação linear dos valores médios esperados, $E(y_i)$. 

Matematicamente, $l^T$ deve pertencer ao espaço-linha de $X$. Apenas estas funções têm estimativas únicas e interpretação científica.

---

## Teorema de Gauss-Markov e Modelo de Aitken

**Teorema de Gauss-Markov:** Se os erros $\epsilon$ têm média zero, variância constante $\sigma^2$ e não são correlacionados ($Var(\epsilon)=\sigma^2 I$), então o estimador de Mínimos Quadrados Ordinários (OLS) é o **BLUE** (Best Linear Unbiased Estimator) para qualquer função estimável $l^T \beta$.

**Modelo de Aitken:** E se os erros *não* forem "bem-comportados"? 

($Var(\epsilon) = \sigma^2 H$, com $H \neq I$).
Neste caso, o estimador BLUE é o de **Mínimos Quadrados Generalizados (GLS)**:
$$ b_{GLS} = (X^T H^{-1} X)^{-} X^T H^{-1} y $$
Ele pondera as observações para dar menos peso às de maior variância.

---
class: center, middle, inverse

## 4. Análise de Variância e Inferência

(Seções 3.10 em diante)

---

## 3.10 A Geometria da Análise de Variância (ANOVA)

A ANOVA particiona a variabilidade total dos dados. A intuição geométrica é a mais poderosa: é uma aplicação do **Teorema de Pitágoras** em um espaço vetorial de $n$ dimensões.

O vetor de dados $y$ é decomposto em componentes **ortogonais**:
$$y = \underbrace{P_{J_n} y}_{\text{Vetor Média}} + \underbrace{(P_X - P_{J_n})y}_{\text{Vetor de Efeitos}} + \underbrace{(I - P_X)y}_{\text{Vetor de Resíduos}}$$
$$y_{ajustado} = \bar{y} \mathbf{1} + \text{efeitos} + \text{resíduos}$$

Como os vetores são ortogonais, a soma de suas "energias" (norma ao quadrado) se conserva:
$$||y - \bar{y}\mathbf{1}||^2 = ||\text{efeitos}||^2 + ||\text{resíduos}||^2$$
$$SQ_{Total} = SQ_{Modelo} + SQ_{Resíduo}$$

---
### Tabela de ANOVA (Corrigida pela Média)


| F.V. | G.L. (df) | S.Q. (Sum Sq) | Q.M. (Mean Sq) | F |
|:---|:---:|:---:|:---:|:---:|
| Tratamento | $t-1$ | $SQ_{Trat}$ | $SQ_{Trat}/(t-1)$ | $QM_{Trat}/QM_{Res}$ |
| Resíduo | $n-t$ | $SQ_{Res}$ | $SQ_{Res}/(n-t)$ | |
| Total | $n-1$ | $SQ_{Total}$ | | |

- **QM (Quadrado Médio):** É uma estimativa da variância associada àquela fonte. $E(QM_{Res}) = \sigma^2$. $E(QM_{Trat}) = \sigma^2 + \frac{k}{t-1}\sum \tau_i^2$.
- **Estatística F:** Compara a variabilidade explicada pelo modelo com a variabilidade residual. Se $H_0$ (não há efeito do tratamento) for verdadeira, $\sum \tau_i^2 = 0$ e esperamos que $F \approx 1$.

---
### Exemplo Prático: ANOVA de um fator com `ggplot2`

**Cenário:** Testando 3 fertilizantes (A, B, C) na produção.
```{r, fig.height=5}
set.seed(123)
dados_fert <- data.frame(
  Fertilizante = factor(rep(c("A", "B", "C"), each = 6)),
  Producao = c(rnorm(6, mean = 20, sd = 2), rnorm(6, mean = 25, sd = 2), rnorm(6, mean = 21, sd = 2))
)

# Visualização completa: pontos de dados + boxplot
p <- ggplot(dados_fert, aes(x = Fertilizante, y = Producao, fill = Fertilizante)) +
  geom_boxplot(alpha = 0.6) +
  geom_jitter(width = 0.1, color = "black") + # Adiciona os pontos de dados
  labs(title = "Produção por Tipo de Fertilizante",
       subtitle = "Boxplots com dados individuais sobrepostos") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
```
---

```{r, fig.height=5}
p
```
---
### Ajustando o Modelo e Gerando a Tabela ANOVA em R

```{r}
modelo_fert <- lm(Producao ~ Fertilizante, data = dados_fert); tabela_anova <- anova(modelo_fert)
print(tabela_anova)
```

**Interpretando o Resultado Detalhadamente:**
1.  **Hipóteses:**
    - $H_0: \mu_A = \mu_B = \mu_C$ (Os fertilizantes não têm efeito diferente).
    - $H_1:$ Pelo menos uma média é diferente.
2.  **Estatística de Teste F:** O valor F de 14.19 indica que a variância *entre* os grupos é ~14 vezes maior que a variância *dentro* dos grupos.
3.  **p-valor:** A probabilidade de observar um F tão grande (ou maior) por puro acaso, *se a H₀ fosse verdadeira*, é de 0.00038.
4.  **Decisão e Conclusão:** Como p < 0.05, rejeitamos $H_0$. Existem evidências estatísticas fortes para concluir que o tipo de fertilizante afeta a produção.

---
class: center, middle, inverse

## 3.11 Teoria Inferencial, Distribuições e Referências

---

### O Modelo Linear Normal e Suas Consequências

A premissa adicional de que os erros seguem uma distribuição Normal, $\epsilon \sim N(0, \sigma^2 I)$, é o que permite a inferência clássica (testes F, testes t, intervalos de confiança).

Sob esta premissa:
- $y \sim N(X\beta, \sigma^2 I)$.
- As Somas de Quadrados, que são formas quadráticas de vetores normais, seguem distribuições Qui-quadrado ($\chi^2$).
- A independência entre as Somas de Quadrados (garantida pelo Teorema de Cochran) permite a construção da estatística $F$.

---

### O Teorema de Cochran: A Grande Unificação

Para qualquer modelo linear (com fatores cruzados, aninhados, etc.), a soma de quadrados total pode ser particionada em componentes ortogonais.

$$ y^T(I-J_n)y = y^T A_1 y + y^T A_2 y + \dots + y^T A_p y $$

O **Teorema de Cochran** garante que, sob a premissa de normalidade e $H_0$:
1.  Cada termo $\frac{y^T A_i y}{\sigma^2}$ segue uma distribuição $\chi^2_{k_i}$, onde $k_i = posto(A_i)$ são os graus de liberdade.
2.  Todos esses termos são **mutuamente independentes**.

Este teorema é a fundação que justifica toda a tabela de ANOVA, permitindo-nos construir testes F válidos para cada linha da tabela.

---

### Referências e Leitura Complementar

Para um aprofundamento nos tópicos discutidos, as seguintes obras são referências clássicas na área de Modelos Lineares:

- **Searle, S. R. (1971). *Linear Models*.** Wiley.
  - *Um texto fundamental e matematicamente rigoroso sobre a teoria de modelos lineares, especialmente para dados desbalanceados.*

- **Scheffé, H. (1959). *The Analysis of Variance*.** Wiley.
  - *O trabalho clássico sobre ANOVA, com uma abordagem geométrica profunda.*

- **Christensen, R. (2011). *Plane Answers to Complex Questions: The Theory of Linear Models*.** Springer.
  - *Uma abordagem moderna e muito conceituada, que conecta a teoria com a prática computacional.*

- **Rao, C. R. (1973). *Linear Statistical Inference and Its Applications*.** Wiley.
  - *Uma referência abrangente sobre inferência estatística, com capítulos detalhados sobre a teoria de estimação em modelos lineares.*
```